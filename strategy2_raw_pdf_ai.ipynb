{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3632fdc6",
   "metadata": {},
   "source": [
    "# 🎯 Strategy 2: Raw PDF + AI Structuring\n",
    "\n",
    "**Philosophy**: Extract raw text with pypdf, then use powerful AI models to structure and parse the content with full control.\n",
    "\n",
    "## Optimization Areas:\n",
    "- Text extraction quality improvement\n",
    "- AI model selection\n",
    "- Prompt engineering for academic structure\n",
    "- Chunking strategy optimization\n",
    "- Page range selection logic\n",
    "\n",
    "## Available Papers:\n",
    "- `30YearsResearchGate.pdf`\n",
    "- `SchenkBekkerSchmitt2025PrecRes.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79bc0b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: together in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (1.4.6)\n",
      "Requirement already satisfied: pypdf in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (5.8.0)\n",
      "Requirement already satisfied: pydantic in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (2.11.7)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (21.0.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (3.12.14)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (4.67.1)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (0.15.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (2.32.4)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (11.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (8.1.8)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (3.18.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pypdf) (4.14.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: together in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (1.4.6)\n",
      "Requirement already satisfied: pypdf in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (5.8.0)\n",
      "Requirement already satisfied: pydantic in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (2.11.7)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (21.0.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (3.12.14)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (4.67.1)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (0.15.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (2.32.4)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (11.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (8.1.8)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (3.18.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pypdf) (4.14.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.20.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from rich<14.0.0,>=13.8.1->together) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from rich<14.0.0,>=13.8.1->together) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/fredygerman/Library/Python/3.9/lib/python/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip3 install together pypdf pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8aa516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fredygerman/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from together import Together\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ee337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys\n",
    "tog_api = \"xxx\"\n",
    "together = Together(api_key=tog_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cbb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ OPTIMIZATION AREA: Data Model Enhancement\n",
    "class ResearchChunk(BaseModel):\n",
    "    section_type: str = Field(description=\"Type of section: abstract, introduction, methodology, results, discussion, conclusion, references, etc.\")\n",
    "    section_number: Optional[str] = Field(description=\"Section number if available (e.g., '2.1', '3.2')\")\n",
    "    page_number: int = Field(description=\"Page number of the chunk\")\n",
    "    content: str = Field(description=\"Parsed content of the chunk\")\n",
    "    is_figure_caption: bool = Field(default=False, description=\"Whether this chunk is a figure caption\")\n",
    "    is_table: bool = Field(default=False, description=\"Whether this chunk contains table data\")\n",
    "    \n",
    "    # 🔧 TRY: Add these optional fields for enhanced academic parsing:\n",
    "    confidence_score: Optional[float] = Field(default=None, description=\"AI confidence in parsing accuracy\")\n",
    "    has_citations: bool = Field(default=False, description=\"Whether chunk contains citations\")\n",
    "    has_equations: bool = Field(default=False, description=\"Whether chunk contains mathematical equations\")\n",
    "    keywords: Optional[list[str]] = Field(default=None, description=\"Key academic terms found in chunk\")\n",
    "    subsection_title: Optional[str] = Field(default=None, description=\"Subsection title if identifiable\")\n",
    "\n",
    "class ResearchPaper(BaseModel):\n",
    "    title: Optional[str] = Field(description=\"Title of the research paper if identifiable\")\n",
    "    authors: Optional[str] = Field(description=\"Authors of the paper if identifiable\")\n",
    "    chunks: list[ResearchChunk] = Field(description=\"List of chunks that build the research paper\")\n",
    "    \n",
    "    # 🔧 TRY: Add these optional fields for enhanced paper metadata:\n",
    "    abstract: Optional[str] = Field(default=None, description=\"Paper abstract if identifiable\")\n",
    "    publication_year: Optional[int] = Field(default=None, description=\"Publication year if found\")\n",
    "    journal: Optional[str] = Field(default=None, description=\"Journal or venue if identifiable\")\n",
    "    doi: Optional[str] = Field(default=None, description=\"DOI if found in paper\")\n",
    "    total_pages: Optional[int] = Field(default=None, description=\"Total number of pages processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfb49cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/papers/30YearsResearchGate.pdf\n",
      "Paper 2: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/papers/SchenkBekkerSchmitt2025PrecRes.pdf\n"
     ]
    }
   ],
   "source": [
    "# Research paper file paths\n",
    "research_paper_1 = \"/Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/papers/30YearsResearchGate.pdf\"\n",
    "research_paper_2 = \"/Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/papers/SchenkBekkerSchmitt2025PrecRes.pdf\"\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"/Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/input_papers/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Paper 1: {research_paper_1}\")\n",
    "print(f\"Paper 2: {research_paper_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2936288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1 characteristics: {'total_pages': 21, 'has_abstract': False, 'has_figures': False, 'has_tables': False, 'complexity': 'high'}\n",
      "Paper 2 characteristics: {'total_pages': 15, 'has_abstract': False, 'has_figures': False, 'has_tables': False, 'complexity': 'medium'}\n"
     ]
    }
   ],
   "source": [
    "# ⚡ OPTIMIZATION AREA 1: Text Extraction Enhancement\n",
    "def get_research_paper_text(pdf_path: str, start_page: int = 1, end_page: Optional[int] = None):\n",
    "    \"\"\"Extract text from research paper with page range\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    if end_page is None:\n",
    "        end_page = total_pages\n",
    "    \n",
    "    print(f\"Total pages in PDF: {total_pages}\")\n",
    "    print(f\"Extracting pages {start_page} to {min(end_page, total_pages)}\")\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_num in range(start_page, min(end_page + 1, total_pages + 1)):\n",
    "        page_text = reader.pages[page_num-1].extract_text()\n",
    "        \n",
    "        # 🔧 TRY: Add text cleaning, normalization, or enhancement here\n",
    "        # Examples:\n",
    "        # page_text = page_text.replace('\\n\\n', '\\n')  # Normalize line breaks\n",
    "        # page_text = re.sub(r'\\s+', ' ', page_text)   # Clean whitespace\n",
    "        # page_text = clean_academic_text(page_text)   # Custom cleaning function\n",
    "        \n",
    "        text += f\"\\n--- PAGE {page_num} ---\\n\" + page_text\n",
    "        \n",
    "    return text\n",
    "\n",
    "def analyze_paper_characteristics(pdf_path: str):\n",
    "    \"\"\"Analyze paper to optimize processing strategy\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    # Sample first page to analyze content\n",
    "    first_page = reader.pages[0].extract_text()\n",
    "    \n",
    "    characteristics = {\n",
    "        'total_pages': total_pages,\n",
    "        'has_abstract': 'abstract' in first_page.lower(),\n",
    "        'has_figures': 'figure' in first_page.lower(),\n",
    "        'has_tables': 'table' in first_page.lower(),\n",
    "        'complexity': 'high' if total_pages > 15 else 'medium' if total_pages > 8 else 'low'\n",
    "    }\n",
    "    \n",
    "    return characteristics\n",
    "\n",
    "# Analyze both papers\n",
    "paper1_analysis = analyze_paper_characteristics(research_paper_1)\n",
    "paper2_analysis = analyze_paper_characteristics(research_paper_2)\n",
    "print(f\"Paper 1 characteristics: {paper1_analysis}\")\n",
    "print(f\"Paper 2 characteristics: {paper2_analysis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d43c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ OPTIMIZATION AREA 2: Advanced Prompt Engineering\n",
    "research_parsing_instruction = \"\"\"\n",
    "The provided document is a research paper. I want you to parse it systematically while preserving the academic structure and important content.\n",
    "\n",
    "IMPORTANT PARSING GUIDELINES:\n",
    "1. Break into MULTIPLE chunks at logical boundaries (paragraphs, sections, subsections)\n",
    "2. Identify section types: abstract, introduction, methodology, results, discussion, conclusion, references\n",
    "3. Preserve figure captions and table data as separate chunks\n",
    "4. Extract section numbers (e.g., \"2.1\", \"3.2\") when available\n",
    "5. Maintain academic formatting and citations\n",
    "6. Filter out headers/footers but keep page numbers\n",
    "7. Preserve mathematical formulas and equations\n",
    "\n",
    "For each chunk:\n",
    "- Identify the section type (abstract, introduction, etc.)\n",
    "- Extract section numbers if available\n",
    "- Identify the page number\n",
    "- Mark if it's a figure caption or table\n",
    "- Extract the content preserving academic language\n",
    "\n",
    "You MUST create multiple chunks - do not combine different sections or paragraphs into one chunk.\n",
    "\"\"\"\n",
    "\n",
    "# 🔧 TRY: Alternative prompt variations:\n",
    "\n",
    "# Option A: More specific academic focus\n",
    "research_parsing_instruction_v2 = \"\"\"\n",
    "You are an expert academic document parser specializing in research papers.\n",
    "Focus on:\n",
    "1. Precise section identification (abstract, intro, methods, results, discussion, conclusion)\n",
    "2. Mathematical equation preservation\n",
    "3. Citation and reference handling\n",
    "4. Figure/table metadata extraction\n",
    "Break into granular chunks maintaining academic integrity.\n",
    "\"\"\"\n",
    "\n",
    "# Option B: Quality-focused approach\n",
    "research_parsing_instruction_v3 = \"\"\"\n",
    "Extract maximum value from this research paper by:\n",
    "1. Identifying key insights and findings\n",
    "2. Preserving methodological details\n",
    "3. Maintaining result data and analysis\n",
    "4. Capturing conclusions and implications\n",
    "Focus on content quality and academic completeness.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e040dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_json_parse(ai_response, schema_class):\n",
    "    \"\"\"\n",
    "    Robust JSON parsing with multiple fallback strategies\n",
    "    \"\"\"\n",
    "    # Check if response is valid\n",
    "    if not ai_response or not ai_response.choices or len(ai_response.choices) == 0:\n",
    "        raise Exception(\"Invalid AI response: no choices available\")\n",
    "    \n",
    "    raw_content = ai_response.choices[0].message.content\n",
    "    \n",
    "    # Check if content is valid\n",
    "    if not raw_content:\n",
    "        raise Exception(\"Invalid AI response: no content available\")\n",
    "    \n",
    "    # Strategy 1: Direct parsing\n",
    "    try:\n",
    "        return schema_class.model_validate_json(raw_content)\n",
    "    except Exception as e1:\n",
    "        print(f\"Strategy 1 failed: {e1}\")\n",
    "    \n",
    "    # Strategy 2: Clean common JSON issues\n",
    "    try:\n",
    "        # Fix escaped quotes in content\n",
    "        cleaned = re.sub(r'(?<!\\\\)\"(?![,:}\\]])', r'\\\\\"', raw_content)\n",
    "        \n",
    "        # Fix newlines in strings\n",
    "        cleaned = re.sub(r'(?<!\\\\)\\n(?![,:}\\]])', r'\\\\n', cleaned)\n",
    "        \n",
    "        # Validate JSON first\n",
    "        json.loads(cleaned)\n",
    "        return schema_class.model_validate_json(cleaned)\n",
    "    except Exception as e2:\n",
    "        print(f\"Strategy 2 failed: {e2}\")\n",
    "    \n",
    "    # Strategy 3: Re-request with stricter format\n",
    "    try:\n",
    "        print(\"🔧 Requesting AI to reformat response...\")\n",
    "        reformat_response = together.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"Fix this JSON by:\n",
    "1. Properly escaping all quotes in content strings\n",
    "2. Removing any trailing commas\n",
    "3. Ensuring valid JSON format\n",
    "4. Keeping all original data intact\n",
    "Return only the corrected JSON.\"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Fix this JSON response:\\n{raw_content[:8000]}\",  # Limit length\n",
    "                },\n",
    "            ],\n",
    "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "            temperature=0.0,\n",
    "            stream=False,\n",
    "        )\n",
    "        \n",
    "        # Check if reformat response is valid\n",
    "        if not reformat_response or not reformat_response.choices or len(reformat_response.choices) == 0:\n",
    "            raise Exception(\"Invalid reformat response: no choices available\")\n",
    "        \n",
    "        reformat_content = reformat_response.choices[0].message.content\n",
    "        if not reformat_content:\n",
    "            raise Exception(\"Invalid reformat response: no content available\")\n",
    "        \n",
    "        return schema_class.model_validate_json(reformat_content)\n",
    "    except Exception as e3:\n",
    "        print(f\"Strategy 3 failed: {e3}\")\n",
    "    \n",
    "    # Strategy 4: Manual fallback\n",
    "    print(\"🔧 Using manual fallback parsing...\")\n",
    "    if schema_class == ResearchPaper:\n",
    "        return ResearchPaper(\n",
    "            title=\"Research Paper (Manual Fallback)\",\n",
    "            authors=\"Unknown (parsing error)\",\n",
    "            chunks=[\n",
    "                ResearchChunk(\n",
    "                    section_type=\"other\",\n",
    "                    page_number=1,\n",
    "                    content=\"Failed to parse content due to JSON formatting issues. Please try again with different parameters.\",\n",
    "                    is_figure_caption=False,\n",
    "                    is_table=False\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    raise Exception(\"All parsing strategies failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f5b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_research_paper(pdf_path: str, output_filename: str, start_page: int = 1, end_page: Optional[int] = None, custom_prompt: str = None):\n",
    "    \"\"\"Process a research paper and save chunks\"\"\"\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    \n",
    "    # 🔧 TRY: Add preprocessing steps here\n",
    "    # Examples:\n",
    "    # - Validate PDF integrity\n",
    "    # - Analyze paper characteristics\n",
    "    # - Choose optimal extraction method\n",
    "    \n",
    "    # Extract text\n",
    "    text = get_research_paper_text(pdf_path, start_page, end_page)\n",
    "    \n",
    "    # 🔧 TRY: Add text cleaning/enhancement before AI processing\n",
    "    # text = preprocess_academic_text(text)\n",
    "    \n",
    "    # Use custom prompt or default\n",
    "    prompt = custom_prompt or research_parsing_instruction\n",
    "    \n",
    "    # ⚡ OPTIMIZATION AREA 3: AI Model and Parameters\n",
    "    # Parse with AI\n",
    "    extract = together.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"{prompt}\n",
    "                \n",
    "CRITICAL JSON FORMATTING RULES:\n",
    "1. Escape all quotes in content with \\\\\"\n",
    "2. Replace newlines in content with \\\\n\n",
    "3. Ensure valid JSON structure\n",
    "4. No trailing commas\n",
    "5. Keep content strings under 1000 characters each\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Create AT LEAST 15-30 chunks from the provided text\n",
    "2. Each paragraph should be its own chunk\n",
    "3. Each section/subsection should be separate chunks\n",
    "4. Identify section types: abstract, introduction, methodology, results, discussion, conclusion\n",
    "5. Extract section numbers (e.g., \"2.1\", \"3.2\") when available\n",
    "6. Mark figure captions and tables separately\n",
    "7. Preserve academic language and citations\n",
    "\n",
    "For each chunk:\n",
    "- section_type: abstract, introduction, methodology, results, discussion, conclusion, references, figure_caption, table, other\n",
    "- section_number: extract from text (e.g., \"2.1\", \"3.2\") or null\n",
    "- page_number: extract from text or estimate\n",
    "- content: the actual text content (PROPERLY ESCAPED and UNDER 1000 chars)\n",
    "- is_figure_caption: true if this is a figure caption\n",
    "- is_table: true if this contains table data\n",
    "\n",
    "CRITICAL: Do NOT merge paragraphs or sections. Each distinct paragraph = one chunk.\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Break this research paper content into many separate chunks:\\n\\n{text[:12000]}\",  # Limit input length\n",
    "            },\n",
    "        ],\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",  # 🔧 TRY: Different models\n",
    "        response_format={\"type\": \"json_object\", \"schema\": ResearchPaper.model_json_schema()},\n",
    "        temperature=0.1,  # 🔧 TRY: Experiment with temperature for consistency vs creativity\n",
    "        max_tokens=4000,  # 🔧 TRY: Adjust based on paper complexity\n",
    "        stream=False,\n",
    "    )\n",
    "    \n",
    "    # Process results with robust parsing\n",
    "    try:\n",
    "        data = robust_json_parse(extract, ResearchPaper)\n",
    "        print(\"✅ Successfully parsed research paper data!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing results: {e}\")\n",
    "        # Create fallback data\n",
    "        data = ResearchPaper(\n",
    "            title=\"Research Paper (Parsing Error)\",\n",
    "            authors=\"Unknown (parsing error)\",\n",
    "            chunks=[]\n",
    "        )\n",
    "    \n",
    "    # 🔧 TRY: Add post-processing validation and enhancement here\n",
    "    # Examples:\n",
    "    # - Validate chunk quality\n",
    "    # - Merge similar chunks\n",
    "    # - Enhance metadata\n",
    "    # data = enhance_research_data(data)\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {data.title or 'Research Paper'}\\n\\n\")\n",
    "        f.write(f\"**Authors:** {data.authors or 'Not identified'}\\n\\n\")\n",
    "        f.write(f\"**Total Chunks:** {len(data.chunks)}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        for i, chunk in enumerate(data.chunks):\n",
    "            f.write(f\"## Chunk {i+1}\\n\\n\")\n",
    "            f.write(f\"- **Section Type:** {chunk.section_type}\\n\")\n",
    "            f.write(f\"- **Section Number:** {chunk.section_number or 'N/A'}\\n\")\n",
    "            f.write(f\"- **Page:** {chunk.page_number}\\n\")\n",
    "            f.write(f\"- **Figure Caption:** {chunk.is_figure_caption}\\n\")\n",
    "            f.write(f\"- **Table:** {chunk.is_table}\\n\\n\")\n",
    "            f.write(f\"**Content:**\\n{chunk.content}\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(data.chunks)} chunks to: {output_path}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8c83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Paper 1 with pages 1-12\n",
      "Processing: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/papers/30YearsResearchGate.pdf\n",
      "Total pages in PDF: 21\n",
      "Extracting pages 1 to 12\n",
      "✅ Successfully parsed research paper data!\n",
      "Saved 15 chunks to: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/input_papers/strategy2_paper1_raw_ai.md\n",
      "✅ Successfully parsed research paper data!\n",
      "Saved 15 chunks to: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/input_papers/strategy2_paper1_raw_ai.md\n"
     ]
    }
   ],
   "source": [
    "# Process first research paper\n",
    "# ⚡ OPTIMIZATION AREA: Smart Page Range Selection\n",
    "# Adapt page range based on paper analysis\n",
    "start_page_1 = 1\n",
    "end_page_1 = 12 if paper1_analysis['complexity'] == 'high' else paper1_analysis['total_pages']\n",
    "\n",
    "print(f\"Processing Paper 1 with pages {start_page_1}-{end_page_1}\")\n",
    "research_data_1 = process_research_paper(\n",
    "    research_paper_1, \n",
    "    \"strategy2_paper1_raw_ai.md\", \n",
    "    start_page_1, \n",
    "    end_page_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01815ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Paper 2 with pages 1-15\n",
      "Processing: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/papers/SchenkBekkerSchmitt2025PrecRes.pdf\n",
      "Total pages in PDF: 15\n",
      "Extracting pages 1 to 15\n",
      "✅ Successfully parsed research paper data!\n",
      "Saved 33 chunks to: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/input_papers/strategy2_paper2_raw_ai.md\n",
      "✅ Successfully parsed research paper data!\n",
      "Saved 33 chunks to: /Users/fredygerman/Personal/builds/exp/twiga-challenge-1/data/input_papers/strategy2_paper2_raw_ai.md\n"
     ]
    }
   ],
   "source": [
    "# Process second research paper\n",
    "start_page_2 = 1\n",
    "end_page_2 = 12 if paper2_analysis['complexity'] == 'high' else paper2_analysis['total_pages']\n",
    "\n",
    "print(f\"Processing Paper 2 with pages {start_page_2}-{end_page_2}\")\n",
    "research_data_2 = process_research_paper(\n",
    "    research_paper_2, \n",
    "    \"strategy2_paper2_raw_ai.md\", \n",
    "    start_page_2, \n",
    "    end_page_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b39090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 STRATEGY 2 ANALYSIS FOR Paper 1 (30YearsResearchGate):\n",
      "Paper Title: 30 Years of Unlocking Sustainable Educational Opportunities through Open and Distance Learning\n",
      "Authors: Martha Jacob Kabate, Upendo Nombo\n",
      "Total chunks: 15\n",
      "Section types: {'abstract': 1, 'introduction': 2, 'concept and definition': 1, 'background of the chapter': 3, 'literature review': 6, 'figure caption': 1, 'other': 1}\n",
      "Figures detected: 1\n",
      "Tables detected: 0\n",
      "Average chunk length: 150 characters\n",
      "Potential citations: 9\n",
      "\n",
      "📊 STRATEGY 2 ANALYSIS FOR Paper 2 (SchenkBekkerSchmitt2025):\n",
      "Paper Title: Chunk 1\n",
      "Authors: \n",
      "Total chunks: 33\n",
      "Section types: {'abstract': 1, 'introduction': 28, 'geological setting': 4}\n",
      "Figures detected: 0\n",
      "Tables detected: 0\n",
      "Average chunk length: 213 characters\n",
      "Potential citations: 40\n"
     ]
    }
   ],
   "source": [
    "# ⚡ OPTIMIZATION AREA 4: Quality Assessment and Analysis\n",
    "def analyze_strategy2_results(research_data, paper_name):\n",
    "    \"\"\"Analyze the quality of Strategy 2 results\"\"\"\n",
    "    \n",
    "    if not research_data.chunks:\n",
    "        print(f\"❌ No chunks found for {paper_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Quality metrics\n",
    "    section_types = {}\n",
    "    figure_count = 0\n",
    "    table_count = 0\n",
    "    total_content_length = 0\n",
    "    citation_count = 0\n",
    "    \n",
    "    for chunk in research_data.chunks:\n",
    "        # Count section types\n",
    "        section_types[chunk.section_type] = section_types.get(chunk.section_type, 0) + 1\n",
    "        \n",
    "        # Count special elements\n",
    "        if chunk.is_figure_caption:\n",
    "            figure_count += 1\n",
    "        if chunk.is_table:\n",
    "            table_count += 1\n",
    "        \n",
    "        # Track content metrics\n",
    "        total_content_length += len(chunk.content)\n",
    "        \n",
    "        # Count potential citations\n",
    "        citation_count += chunk.content.count('(') + chunk.content.count('[')\n",
    "    \n",
    "    avg_chunk_length = total_content_length // len(research_data.chunks) if research_data.chunks else 0\n",
    "    \n",
    "    print(f\"\\n📊 STRATEGY 2 ANALYSIS FOR {paper_name}:\")\n",
    "    print(f\"Paper Title: {research_data.title}\")\n",
    "    print(f\"Authors: {research_data.authors}\")\n",
    "    print(f\"Total chunks: {len(research_data.chunks)}\")\n",
    "    print(f\"Section types: {section_types}\")\n",
    "    print(f\"Figures detected: {figure_count}\")\n",
    "    print(f\"Tables detected: {table_count}\")\n",
    "    print(f\"Average chunk length: {avg_chunk_length} characters\")\n",
    "    print(f\"Potential citations: {citation_count}\")\n",
    "    \n",
    "    return {\n",
    "        'chunks': len(research_data.chunks),\n",
    "        'sections': section_types,\n",
    "        'figures': figure_count,\n",
    "        'tables': table_count,\n",
    "        'avg_length': avg_chunk_length,\n",
    "        'citations': citation_count\n",
    "    }\n",
    "\n",
    "# Analyze both papers\n",
    "results_1 = analyze_strategy2_results(research_data_1, \"Paper 1 (30YearsResearchGate)\")\n",
    "results_2 = analyze_strategy2_results(research_data_2, \"Paper 2 (SchenkBekkerSchmitt2025)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f08b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Strategy 2 Optimization Workspace\n",
      "\n",
      "Optimization Areas for Raw PDF + AI:\n",
      "☐ Text extraction enhancement (different PDF libraries)\n",
      "☐ AI model selection and parameters\n",
      "☐ Prompt engineering for better structure\n",
      "☐ Adaptive page range selection\n",
      "☐ Post-processing and validation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Your optimized implementation:\\n\\ndef your_optimized_text_extraction(pdf_path):\\n    # Try different PDF libraries:\\n    # - pdfplumber for better table/layout handling\\n    # - PyMuPDF (fitz) for complex layouts\\n    # - Custom preprocessing for academic papers\\n    return extracted_text\\n\\ndef your_optimized_ai_processing(text):\\n    # Optimize AI parameters:\\n    extract = together.chat.completions.create(\\n        messages=[\\n            {\\n                \"role\": \"system\",\\n                \"content\": \"YOUR_OPTIMIZED_PROMPT\",\\n            },\\n            {\"role\": \"user\", \"content\": text},\\n        ],\\n        model=\"YOUR_CHOSEN_MODEL\",  # Try different models\\n        temperature=YOUR_TEMP,  # Optimize for consistency vs creativity\\n        max_tokens=YOUR_TOKENS,  # Based on paper complexity\\n        top_p=YOUR_TOP_P,  # Response diversity\\n        stream=False,\\n    )\\n    return extract\\n\\n# Process both papers with your optimizations\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🎯 YOUR OPTIMIZATION WORKSPACE\n",
    "# Implement your Strategy 2 optimizations here\n",
    "\n",
    "print(\"🚀 Strategy 2 Optimization Workspace\")\n",
    "print(\"\\nOptimization Areas for Raw PDF + AI:\")\n",
    "print(\"☐ Text extraction enhancement (different PDF libraries)\")\n",
    "print(\"☐ AI model selection and parameters\")\n",
    "print(\"☐ Prompt engineering for better structure\")\n",
    "print(\"☐ Adaptive page range selection\")\n",
    "print(\"☐ Post-processing and validation\")\n",
    "\n",
    "# TODO: Add your optimized Raw PDF + AI implementation here\n",
    "\n",
    "# Example optimization template:\n",
    "'''\n",
    "# Your optimized implementation:\n",
    "\n",
    "def your_optimized_text_extraction(pdf_path):\n",
    "    # Try different PDF libraries:\n",
    "    # - pdfplumber for better table/layout handling\n",
    "    # - PyMuPDF (fitz) for complex layouts\n",
    "    # - Custom preprocessing for academic papers\n",
    "    return extracted_text\n",
    "\n",
    "def your_optimized_ai_processing(text):\n",
    "    # Optimize AI parameters:\n",
    "    extract = together.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"YOUR_OPTIMIZED_PROMPT\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        model=\"YOUR_CHOSEN_MODEL\",  # Try different models\n",
    "        temperature=YOUR_TEMP,  # Optimize for consistency vs creativity\n",
    "        max_tokens=YOUR_TOKENS,  # Based on paper complexity\n",
    "        top_p=YOUR_TOP_P,  # Response diversity\n",
    "        stream=False,\n",
    "    )\n",
    "    return extract\n",
    "\n",
    "# Process both papers with your optimizations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1360509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 STRATEGY 2: RAW PDF + AI RESULTS\n",
      "==================================================\n",
      "\n",
      "Paper 1 Results:\n",
      "  Total Chunks: 15\n",
      "  Section Types: 7\n",
      "  Figures/Tables: 1\n",
      "  Avg Chunk Length: 150 chars\n",
      "\n",
      "Paper 2 Results:\n",
      "  Total Chunks: 33\n",
      "  Section Types: 3\n",
      "  Figures/Tables: 0\n",
      "  Avg Chunk Length: 213 chars\n",
      "\n",
      "🎯 Next Steps:\n",
      "1. Review the generated chunk files\n",
      "2. Implement your optimizations above\n",
      "3. Test different PDF extraction methods\n",
      "4. Compare with other strategies\n",
      "5. Document your improvements\n"
     ]
    }
   ],
   "source": [
    "# 📊 STRATEGY 2 RESULTS SUMMARY\n",
    "print(\"🏆 STRATEGY 2: RAW PDF + AI RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if results_1:\n",
    "    print(f\"\\nPaper 1 Results:\")\n",
    "    print(f\"  Total Chunks: {results_1['chunks']}\")\n",
    "    print(f\"  Section Types: {len(results_1['sections'])}\")\n",
    "    print(f\"  Figures/Tables: {results_1['figures'] + results_1['tables']}\")\n",
    "    print(f\"  Avg Chunk Length: {results_1['avg_length']} chars\")\n",
    "\n",
    "if results_2:\n",
    "    print(f\"\\nPaper 2 Results:\")\n",
    "    print(f\"  Total Chunks: {results_2['chunks']}\")\n",
    "    print(f\"  Section Types: {len(results_2['sections'])}\")\n",
    "    print(f\"  Figures/Tables: {results_2['figures'] + results_2['tables']}\")\n",
    "    print(f\"  Avg Chunk Length: {results_2['avg_length']} chars\")\n",
    "\n",
    "print(\"\\n🎯 Next Steps:\")\n",
    "print(\"1. Review the generated chunk files\")\n",
    "print(\"2. Implement your optimizations above\")\n",
    "print(\"3. Test different PDF extraction methods\")\n",
    "print(\"4. Compare with other strategies\")\n",
    "print(\"5. Document your improvements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
